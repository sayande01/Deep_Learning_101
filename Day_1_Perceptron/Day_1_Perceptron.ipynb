{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep Learning Tutorial - Day 1**\n",
        "## **Introduction to Deep Learning**\n",
        "\n",
        "Welcome to Day 1 of the Deep Learning journey! In this notebook, we will explore the basics of Deep Learning, understand the mathematical foundation of perceptrons, and implement a simple perceptron in Python using NumPy.\n",
        "\n",
        "---\n",
        "\n",
        "## **What is Deep Learning?**\n",
        "\n",
        "Deep Learning is a subset of machine learning that focuses on neural networks with multiple layers. Unlike traditional machine learning, deep learning excels at feature extraction and representation learning from raw data, such as images, text, and audio.\n",
        "\n",
        "### **Comparison: Deep Learning vs. Machine Learning**\n",
        "| Aspect                | Machine Learning                 | Deep Learning                    |\n",
        "|-----------------------|----------------------------------|----------------------------------|\n",
        "| Feature Engineering   | Requires manual intervention    | Automated feature learning       |\n",
        "| Data Requirements     | Works well with smaller datasets | Requires large datasets          |\n",
        "| Processing Power      | Moderate requirements           | High computational requirements  |\n",
        "| Performance           | Good for structured data        | Excels with unstructured data    |\n",
        "\n",
        "---\n",
        "\n",
        "## **Key Terminologies**\n",
        "1. **Neural Network**: A computational model inspired by the human brain, consisting of neurons (nodes) connected by weights.\n",
        "2. **Layers**: Neural networks are made up of input, hidden, and output layers.\n",
        "3. **Weights**: Parameters that determine the strength of the connection between neurons.\n",
        "4. **Activation Functions**: Functions that introduce non-linearity into the model (e.g., Sigmoid, ReLU).\n",
        "\n",
        "---\n",
        "\n",
        "## **Mathematics Behind Perceptrons**\n",
        "\n",
        "A perceptron is the simplest type of neural network, consisting of:\n",
        "1. **Inputs**: x1, x2, ..., xn\n",
        "2. **Weights**: w1, w2, ..., wn\n",
        "3. **Bias**: b (adjusts the activation threshold)\n",
        "4. **Output**: The output is activated using an activation function.\n",
        "\n",
        "### **Perceptron Formula**\n",
        "The perceptron computes the following:\n",
        "\n",
        "z = (w1 * x1) + (w2 * x2) + ... + (wn * xn) + b\n",
        "\n",
        "The output is passed through an activation function f(z), such as Sigmoid or ReLU, to determine the final result.\n",
        "\n",
        "---\n",
        "\n",
        "## **Common Activation Functions**\n",
        "\n",
        "1. **Sigmoid Function**:\n",
        "   f(z) = 1 / (1 + e^(-z))\n",
        "\n",
        "   - **Range**: (0, 1)\n",
        "   - **Use case**: Outputs probabilities.\n",
        "\n",
        "2. **ReLU (Rectified Linear Unit)**:\n",
        "   f(z) = max(0, z)\n",
        "\n",
        "   - **Range**: [0, infinity)\n",
        "   - **Use case**: Avoids vanishing gradients and is computationally efficient.\n",
        "\n",
        "---\n",
        "\n",
        "## **Advantages and Disadvantages of Perceptrons**\n",
        "\n",
        "### **Advantages**\n",
        "1. Simple to understand and implement.\n",
        "2. Forms the basis of more complex neural networks.\n",
        "\n",
        "### **Disadvantages**\n",
        "1. Cannot solve non-linear problems (e.g., XOR).\n",
        "2. Requires an activation function for deeper learning tasks.\n"
      ],
      "metadata": {
        "id": "0Yre3EWvmwfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "\n",
        "# Define the perceptron function\n",
        "def perceptron(x, w, b, activation='step'):\n",
        "    \"\"\"\n",
        "    x: Input array (numpy array)\n",
        "    w: Weights array (numpy array)\n",
        "    b: Bias (float)\n",
        "    activation: Activation function ('step', 'sigmoid', 'relu')\n",
        "    \"\"\"\n",
        "    # Calculate the weighted sum\n",
        "    z = np.dot(x, w) + b\n",
        "\n",
        "    # Apply the activation function\n",
        "    if activation == 'step':\n",
        "        return 1 if z >= 0 else 0\n",
        "    elif activation == 'sigmoid':\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    elif activation == 'relu':\n",
        "        return max(0, z)\n",
        "\n",
        "# Inputs and weights\n",
        "x = np.array([2, 3])  # Example inputs\n",
        "w = np.array([0.5, -0.5])  # Example weights\n",
        "b = -1  # Example bias\n",
        "\n",
        "# Test the perceptron with step activation\n",
        "output = perceptron(x, w, b, activation='step')\n",
        "print(\"Step Activation Output:\", output)\n",
        "\n",
        "# Test the perceptron with sigmoid activation\n",
        "output = perceptron(x, w, b, activation='sigmoid')\n",
        "print(\"Sigmoid Activation Output:\", output)\n",
        "\n",
        "# Test the perceptron with ReLU activation\n",
        "output = perceptron(x, w, b, activation='relu')\n",
        "print(\"ReLU Activation Output:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpDEpATJmxAj",
        "outputId": "ff3238a6-616e-4eb6-e32b-42047036ffaf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step Activation Output: 0\n",
            "Sigmoid Activation Output: 0.18242552380635635\n",
            "ReLU Activation Output: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Calculate\n",
        "z = (0.5 * 2) + (-0.5 * 3) + (-1)\n",
        "\n",
        "z = 1 - 1.5 - 1\n",
        "\n",
        "z = -1.5\n",
        "\n",
        "### Step 2: Apply Activation Functions\n",
        "\n",
        "#### Step Activation Function:\n",
        "\n",
        "f(z) =\n",
        "- 1 if z >= 0\n",
        "- 0 else\n",
        "\n",
        "Since z = -1.5, f(z) = 0.\n",
        "\n",
        "#### Sigmoid Activation Function:\n",
        "\n",
        "f(z) = 1 / (1 + e^(-z))\n",
        "\n",
        "f(z) = 1 / (1 + e^(1.5))\n",
        "\n",
        "f(z) = 1 / (1 + 4.4817)\n",
        "\n",
        "f(z) ≈ 1 / 5.4817 ≈ 0.1824\n",
        "\n",
        "#### ReLU Activation Function:\n",
        "\n",
        "f(z) = max(0, z)\n",
        "\n",
        "f(z) = max(0, -1.5) = 0\n"
      ],
      "metadata": {
        "id": "xLmVX8Ohnnni"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vPVC6e77m0tW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}