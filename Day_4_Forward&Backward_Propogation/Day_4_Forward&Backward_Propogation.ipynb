{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Day 4: Forward and Backpropagation**\n",
        "## **Objective**\n",
        "1. Understand the step-by-step process of forward propagation.\n",
        "2. Learn how the chain rule is applied in backpropagation.\n",
        "3. Implement manual forward and backward passes for a simple neural network.\n",
        "---\n",
        "\n",
        "## **1. What is Forward Propagation?**\n",
        "Forward propagation is the process of passing the input data through the layers of a neural network to calculate the output.\n",
        "\n",
        "### **Steps in Forward Propagation**\n",
        "1. **Input Data**: Begin with input values (features of the data).\n",
        "2. **Weighted Sum (Linear Transformation)**:  \n",
        "   Compute `z = W*x + b`, where:\n",
        "   - `W` is the weight matrix,\n",
        "   - `x` is the input vector,\n",
        "   - `b` is the bias term.\n",
        "3. **Activation Function**: Apply an activation function to introduce non-linearity:  \n",
        "   `a = Activation(z)`\n",
        "4. **Output**: Repeat the above steps for all layers until the final output is computed."
      ],
      "metadata": {
        "id": "ISVpFSMny1LG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. What is Backward Propagation?**\n",
        "Backward propagation calculates the gradients of the loss function with respect to the weights and biases using the **chain rule** of calculus. These gradients are used to update the weights and biases during training.\n",
        "\n",
        "### **Steps in Backward Propagation**\n",
        "1. **Compute Loss**: Measure the difference between predicted output and actual target using a loss function.\n",
        "2. **Calculate Gradients**: Use the chain rule to compute:\n",
        "   - `∂L/∂a` (loss with respect to activation output),\n",
        "   - `∂a/∂z` (activation output with respect to linear transformation),\n",
        "   - `∂z/∂W` and `∂z/∂b` (linear transformation with respect to weights and biases).\n",
        "3. **Update Weights and Biases**: Adjust `W` and `b` to minimize the loss using:\n",
        "   - `W = W - learning_rate * ∂L/∂W`\n",
        "   - `b = b - learning_rate * ∂L/∂b`"
      ],
      "metadata": {
        "id": "NBzHYb0fy8RG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Day 4: Forward and Backpropagation**\n",
        "## **Objective**\n",
        "1. Understand the step-by-step process of forward propagation.\n",
        "2. Learn how the chain rule is applied in backpropagation.\n",
        "3. Implement manual forward and backward passes for a simple neural network.\n",
        "---\n",
        "\n",
        "## **1. What is Forward Propagation?**\n",
        "Forward propagation is the process of passing the input data through the layers of a neural network to calculate the output.\n",
        "\n",
        "### **Steps in Forward Propagation**\n",
        "1. **Input Data**: Begin with input values (features of the data).\n",
        "2. **Weighted Sum (Linear Transformation)**:  \n",
        "   Compute `z = W*x + b`, where:\n",
        "   - `W` is the weight matrix,\n",
        "   - `x` is the input vector,\n",
        "   - `b` is the bias term.\n",
        "3. **Activation Function**: Apply an activation function to introduce non-linearity:  \n",
        "   `a = Activation(z)`\n",
        "4. **Output**: Repeat the above steps for all layers until the final output is computed.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. What is Backward Propagation?**\n",
        "Backward propagation calculates the gradients of the loss function with respect to the weights and biases using the **chain rule** of calculus. These gradients are used to update the weights and biases during training.\n",
        "\n",
        "### **Steps in Backward Propagation**\n",
        "1. **Compute Loss**: Measure the difference between predicted output and actual target using a loss function.\n",
        "2. **Calculate Gradients**: Use the chain rule to compute:\n",
        "   - `∂L/∂a` (loss with respect to activation output),\n",
        "   - `∂a/∂z` (activation output with respect to linear transformation),\n",
        "   - `∂z/∂W` and `∂z/∂b` (linear transformation with respect to weights and biases).\n",
        "3. **Update Weights and Biases**: Adjust `W` and `b` to minimize the loss using:\n",
        "   - `W = W - learning_rate * ∂L/∂W`\n",
        "   - `b = b - learning_rate * ∂L/∂b`\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Forward Propagation Step-by-Step**\n",
        "### **Step 1: Input Data**\n",
        "The input is a vector or matrix that represents the features of your data.  \n",
        "Example:  \n",
        "    X = [[0.5], [1.0]] (2 features for a single example)\n",
        "\n",
        "### **Step 2: Weighted Sum**\n",
        "Compute the linear combination of weights, inputs, and bias:  \n",
        "    z = W*x + b  \n",
        "Example:  \n",
        "    W = [[0.2, 0.8]] (weights for 2 inputs)  \n",
        "    b = [[0.5]] (bias term)  \n",
        "    z = (0.2 * 0.5) + (0.8 * 1.0) + 0.5  \n",
        "\n",
        "### **Step 3: Activation Function**\n",
        "Apply an activation function to `z` to compute the output of the layer:  \n",
        "    a = sigmoid(z)  \n",
        "Where:  \n",
        "    sigmoid(z) = 1 / (1 + e^(-z))\n",
        "\n",
        "### **Step 4: Output**\n",
        "The final output of the network after all layers.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Backward Propagation Step-by-Step**\n",
        "### **Step 1: Compute Loss**\n",
        "The loss measures the difference between the predicted output and the target value.  \n",
        "Example (Mean Squared Error Loss):  \n",
        "    L = (1/n) * Σ(y_true - y_pred)^2\n",
        "\n",
        "### **Step 2: Calculate Gradients**\n",
        "Use the **chain rule** to compute partial derivatives for weight and bias updates:  \n",
        "1. `∂L/∂a` (loss with respect to activation):  \n",
        "    ∂L/∂a = -(y_true - a)  \n",
        "2. `∂a/∂z` (activation with respect to z):  \n",
        "    For Sigmoid: ∂a/∂z = a * (1 - a)  \n",
        "3. `∂z/∂W` (z with respect to weights):  \n",
        "    ∂z/∂W = x.T\n",
        "\n",
        "### **Step 3: Update Weights and Biases**\n",
        "Using gradient descent:  \n",
        "    W = W - learning_rate * ∂L/∂W  \n",
        "    b = b - learning_rate * ∂L/∂b  \n",
        "\n",
        "---\n",
        "\n",
        "## **5. Code: Forward Propagation**"
      ],
      "metadata": {
        "id": "LnjC1TVQzEuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Input data\n",
        "X = np.array([[0.5], [1.0]])  # Input vector with 2 features\n",
        "\n",
        "# Weights and Bias\n",
        "W = np.array([[0.2, 0.8]])  # Weight matrix\n",
        "b = np.array([[0.5]])       # Bias term\n",
        "\n",
        "# Sigmoid Activation Function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Forward Propagation\n",
        "z = np.dot(W, X) + b  # Linear transformation\n",
        "a = sigmoid(z)        # Activation output\n",
        "\n",
        "print(\"Linear Output (z):\", z)\n",
        "print(\"Activation Output (a):\", a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkSA5q1Qy2fi",
        "outputId": "3bb8ec82-0189-48ff-a2ac-c45455cf7b14"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Output (z): [[1.4]]\n",
            "Activation Output (a): [[0.80218389]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Code: Backward Propagation**"
      ],
      "metadata": {
        "id": "aajdo4EUzZs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Target value (Ground truth)\n",
        "y_true = np.array([[1]])  # True label\n",
        "\n",
        "# Loss function (Mean Squared Error)\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred)**2)\n",
        "\n",
        "# Derivative of the loss with respect to activation\n",
        "def mse_loss_derivative(y_true, y_pred):\n",
        "    return -(y_true - y_pred)\n",
        "\n",
        "# Backward Propagation\n",
        "loss = mse_loss(y_true, a)  # Compute loss\n",
        "dl_da = mse_loss_derivative(y_true, a)  # Derivative of loss wrt activation\n",
        "da_dz = a * (1 - a)  # Derivative of sigmoid\n",
        "dz_dw = X.T  # Derivative of z wrt weights\n",
        "\n",
        "# Gradients using the chain rule\n",
        "dl_dz = dl_da * da_dz  # Gradient wrt z\n",
        "dl_dw = dl_dz * X.T  # Gradient wrt weights (broadcasting)\n",
        "dl_db = dl_dz  # Gradient wrt bias\n",
        "\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Gradient wrt Weights (dl/dW):\", dl_dw)\n",
        "print(\"Gradient wrt Bias (dl/db):\", dl_db)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c320FkgqzV9g",
        "outputId": "3aa68212-6d15-4b61-ac4a-d6eb6a3e845e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.03913121394580363\n",
            "Gradient wrt Weights (dl/dW): [[-0.01569521 -0.03139043]]\n",
            "Gradient wrt Bias (dl/db): [[-0.03139043]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Updating Weight**"
      ],
      "metadata": {
        "id": "YzqSiRxw0PRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Inputs (features)\n",
        "X = np.array([[0.5, 1.5]])  # Shape (1, 2)\n",
        "\n",
        "# Target value (Ground truth)\n",
        "y_true = np.array([[1]])  # True label\n",
        "\n",
        "# Initial weights and bias\n",
        "W = np.array([[0.1, 0.2]])  # Shape (1, 2)\n",
        "b = np.array([[0.3]])       # Shape (1, 1)\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Activation function (Sigmoid)\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Loss function (Mean Squared Error)\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred)**2)\n",
        "\n",
        "# Derivative of the loss with respect to activation\n",
        "def mse_loss_derivative(y_true, y_pred):\n",
        "    return -(y_true - y_pred)\n",
        "\n",
        "# --- Forward Propagation ---\n",
        "z = np.dot(W, X.T) + b  # Weighted sum (Shape: (1, 1))\n",
        "a = sigmoid(z)          # Activation output (Shape: (1, 1))\n",
        "\n",
        "# Compute loss\n",
        "loss = mse_loss(y_true, a)\n",
        "\n",
        "# --- Backward Propagation ---\n",
        "dl_da = mse_loss_derivative(y_true, a)  # Derivative of loss wrt activation\n",
        "da_dz = a * (1 - a)                     # Derivative of sigmoid\n",
        "dl_dz = dl_da * da_dz                   # Derivative of loss wrt z\n",
        "dz_dw = X.T                             # Derivative of z wrt weights\n",
        "\n",
        "dl_dw = dl_dz * dz_dw.T                 # Gradient wrt weights (Shape: (1, 2))\n",
        "dl_db = dl_dz                           # Gradient wrt bias (Shape: (1, 1))\n",
        "\n",
        "# --- Update Weights and Bias ---\n",
        "W = W - learning_rate * dl_dw  # Update weights\n",
        "b = b - learning_rate * dl_db  # Update bias\n",
        "\n",
        "# --- Results ---\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Updated Weights (W):\", W)\n",
        "print(\"Updated Bias (b):\", b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjQa1m6dzoa-",
        "outputId": "0302f9b6-ad83-4f74-ddc8-8e42f0b27999"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.11764182271544733\n",
            "Updated Weights (W): [[0.10038646 0.20115938]]\n",
            "Updated Bias (b): [[0.30077292]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conclusion**\n",
        "\n",
        "In this notebook, we explored the fundamental concepts of **forward and backward propagation**—the core processes that enable neural networks to learn from data. By understanding these steps, we demystified how neural networks calculate predictions, compute errors, and adjust their internal parameters (weights and biases) to minimize the error over time. This iterative process is the foundation of machine learning and deep learning.\n",
        "\n",
        "We started by implementing a single-layer neural network with a sigmoid activation function and learned how to:\n",
        "1. Perform forward propagation to compute predictions.\n",
        "2. Calculate the loss using the Mean Squared Error (MSE) loss function.\n",
        "3. Use **backward propagation** and the **chain rule** to compute gradients.\n",
        "4. Update the weights and biases using **Gradient Descent**.\n",
        "\n",
        "This hands-on exercise demonstrated how a neural network \"learns\" by repeatedly adjusting its parameters to reduce the error and improve predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Learnings**\n",
        "\n",
        "1. **Forward Propagation**:\n",
        "   - Neural networks compute predictions by applying weights, biases, and activation functions to the input data.\n",
        "   - The activation function introduces non-linearity, enabling the network to model complex patterns.\n",
        "\n",
        "2. **Backward Propagation**:\n",
        "   - Using the **chain rule**, gradients are propagated backward through the network to calculate how much each parameter contributes to the error.\n",
        "   - The computed gradients guide the updates to the weights and biases.\n",
        "\n",
        "3. **Gradient Descent**:\n",
        "   - Gradients are used to iteratively adjust the weights and biases in the direction that reduces the loss.\n",
        "   - The **learning rate** plays a crucial role in determining the step size for these updates.\n",
        "\n",
        "4. **The Role of Activation Functions**:\n",
        "   - The sigmoid activation function outputs values between 0 and 1, making it suitable for binary classification tasks.\n",
        "   - Understanding how activation functions and their derivatives influence learning is essential.\n",
        "\n",
        "5. **The Chain Rule in Action**:\n",
        "   - Backward propagation is an excellent example of the **chain rule** in calculus, which allows us to compute gradients for composite functions.\n",
        "\n",
        "6. **Iterative Learning**:\n",
        "   - Learning in neural networks is an iterative process where the parameters are updated repeatedly over multiple epochs until the model converges to a solution.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Is This Important?**\n",
        "\n",
        "Understanding forward and backward propagation is critical for anyone working in deep learning because:\n",
        "- It provides insight into the inner workings of neural networks.\n",
        "- It allows you to debug models effectively by understanding where errors might occur.\n",
        "- It sets the foundation for more advanced topics, such as optimization techniques, advanced architectures (e.g., CNNs, RNNs), and modern frameworks like TensorFlow or PyTorch."
      ],
      "metadata": {
        "id": "QiPiEBLY00B7"
      }
    }
  ]
}