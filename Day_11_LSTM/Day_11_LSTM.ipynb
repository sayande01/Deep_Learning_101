{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Long Short-Term Memory (LSTM)**\n",
        "\n",
        "---\n",
        "\n",
        "## **Notebook Structure**\n",
        "\n",
        "1. **Metadata**\n",
        "2. **Title and Objective**\n",
        "3. **Acknowledgement**\n",
        "4. **Exploratory Data Analysis (EDA)**\n",
        "5. **Concept Explanation**\n",
        "   - What is LSTM?\n",
        "   - Why Use LSTM?\n",
        "   - LSTM Mechanics\n",
        "6. **Mathematical Intuition (Simplified)**\n",
        "7. **Advantages and Disadvantages**\n",
        "8. **Model Implementation**\n",
        "   - Data Preparation\n",
        "   - Building the Model\n",
        "   - Training the Model\n",
        "9. **Model Performance Evaluation**\n",
        "   - Accuracy and Loss Curves\n",
        "   - Predictions and Metrics\n",
        "10. **Learnings and Conclusion**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Metadata**\n",
        "\n",
        "- **Dataset**: IMDb Large Movie Review Dataset\n",
        "- **Task**: Sentiment Analysis\n",
        "- **Tech Stack**: TensorFlow, Keras, Python\n",
        "- **Model**: Long Short-Term Memory (LSTM)\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Title and Objective**\n",
        "\n",
        "### **Title**\n",
        "**Sentiment Analysis using LSTM on IMDb Dataset**\n",
        "\n",
        "### **Objective**\n",
        "To understand and implement Long Short-Term Memory (LSTM) networks for sentiment analysis on the IMDb dataset and evaluate the model's performance with visualizations and metrics.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Acknowledgement**\n",
        "The dataset is sourced from the [Keras IMDb dataset](https://keras.io/api/datasets/imdb/). Special thanks to the creators for making this dataset accessible for research and educational purposes.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Exploratory Data Analysis (EDA)**\n",
        "\n",
        "# **Comprehensive Guide to Long Short-Term Memory (LSTM)**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Introduction**\n",
        "\n",
        "### **1.1 What is LSTM?**\n",
        "Long Short-Term Memory (LSTM) networks are an advanced type of Recurrent Neural Networks (RNNs), introduced in 1997 by Hochreiter and Schmidhuber. They are specifically designed to solve the problems of traditional RNNs, such as the inability to learn long-term dependencies effectively.\n",
        "\n",
        "#### **Key Features**\n",
        "- LSTMs use **memory cells** to store information for longer durations.\n",
        "- They rely on **gates** (Forget Gate, Input Gate, and Output Gate) to control the flow of information:\n",
        "  - **Forget Gate** decides what to remove from memory.\n",
        "  - **Input Gate** determines what new information to add.\n",
        "  - **Output Gate** controls what information is passed to the next layer or step.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.2 Why Use LSTMs?**\n",
        "Traditional RNNs face two major problems:\n",
        "1. **Vanishing Gradients**: RNNs fail to retain information from earlier steps in long sequences because the signal diminishes as it travels backward during training.\n",
        "2. **Exploding Gradients**: Sometimes, the signal grows uncontrollably, making the learning unstable.\n",
        "\n",
        "LSTMs solve these problems by:\n",
        "- Using memory cells to preserve information over time.\n",
        "- Adding gates to selectively update or forget information.\n",
        "\n",
        "#### **Applications of LSTMs**\n",
        "- **Natural Language Processing (NLP)**: Sentiment analysis, machine translation, text summarization.\n",
        "- **Speech Recognition**: Transcribing audio into text.\n",
        "- **Time-Series Forecasting**: Predicting stock prices, energy consumption, etc.\n",
        "- **Video Analysis**: Identifying actions or patterns in video sequences.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How LSTMs Work**\n",
        "\n",
        "### **2.1 Architecture Overview**\n",
        "LSTM cells are the building blocks of LSTM networks. Each cell contains three key gates:\n",
        "1. **Forget Gate**: This decides what past information to discard. For example:\n",
        "   - \"Forget the context of the previous sentence if it’s no longer relevant.\"\n",
        "2. **Input Gate**: This decides what new information to add to the memory.\n",
        "3. **Output Gate**: This decides what information to send to the next step or layer.\n",
        "\n",
        "---\n",
        "\n",
        "### **2.2 LSTM Gates in Simple Terms**\n",
        "\n",
        "#### **Forget Gate**\n",
        "The forget gate filters out irrelevant information:\n",
        "- It looks at the current input and the previous output (hidden state).\n",
        "- Based on this, it decides which parts of the memory to keep or forget.\n",
        "- For example: *\"Forget the irrelevant details of the sentence before.\"*\n",
        "\n",
        "#### **Input Gate**\n",
        "The input gate decides what new information to add to the memory:\n",
        "- It checks the current input and the previous output.\n",
        "- Then it determines what should be updated or added to the memory.\n",
        "- For example: *\"Add the new context of the current word to the memory.\"*\n",
        "\n",
        "#### **Memory Update**\n",
        "The memory cell is updated by:\n",
        "1. Forgetting irrelevant information (Forget Gate).\n",
        "2. Adding new relevant information (Input Gate).\n",
        "\n",
        "#### **Output Gate**\n",
        "The output gate determines what part of the memory should be passed to the next step:\n",
        "- It uses the updated memory and decides what’s relevant for the next layer or step.\n",
        "- For example: *\"Output the current context to understand the sentence better.\"*\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Advantages and Disadvantages of LSTMs**\n",
        "\n",
        "### **3.1 Advantages**\n",
        "1. **Handles Long-Term Dependencies**: Retains relevant information over long sequences.\n",
        "2. **Selective Information Flow**: Gates ensure that only important information is stored and propagated.\n",
        "3. **Wide Applicability**: Useful for tasks like NLP, speech recognition, and time-series forecasting.\n",
        "\n",
        "### **3.2 Disadvantages**\n",
        "1. **Computationally Expensive**: LSTMs are more resource-intensive compared to traditional RNNs.\n",
        "2. **Slower Training**: The added complexity of gates increases the training time.\n",
        "3. **Overfitting**: Without sufficient data, LSTMs can overfit on training data.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Code: Implementing an LSTM for Sentiment Analysis**\n",
        "\n",
        "### **4.1 Dataset**\n",
        "We will use the IMDb dataset, a standard dataset for sentiment analysis tasks. The goal is to classify movie reviews as positive or negative.\n",
        "\n"
      ],
      "metadata": {
        "id": "ViORYStAqmXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "C-DWHWDpWkUZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Concept Explanation**\n",
        "\n",
        "#### **5.1 What is LSTM?**\n",
        "Long Short-Term Memory (LSTM) networks are a specialized type of Recurrent Neural Network (RNN) designed to handle sequential data while mitigating the vanishing gradient problem. Traditional RNNs suffer from short memory due to exponential decay in gradients, making it difficult to learn long-range dependencies. LSTMs address this issue using **gated mechanisms** that regulate the flow of information.\n",
        "\n",
        "An LSTM cell consists of:\n",
        "- A **Forget Gate** to decide which past information should be discarded.\n",
        "- An **Input Gate** to determine what new information should be stored in the cell state.\n",
        "- A **Cell State** that carries long-term memory across time steps.\n",
        "- An **Output Gate** to control what information should be passed to the next hidden state.\n",
        "\n",
        "#### **5.2 Why Use LSTM?**\n",
        "LSTMs are widely used in deep learning applications where sequential dependencies exist. Key reasons for their use include:\n",
        "- **Handles Long-Term Dependencies:** Unlike standard RNNs, LSTMs retain relevant information over long sequences, making them ideal for time-series and NLP tasks.\n",
        "- **Gate Mechanisms:** The **forget, input, and output gates** help selectively store and discard information, preventing the model from accumulating irrelevant past data.\n",
        "- **Widely Used in Applications:**  \n",
        "  - **Natural Language Processing (NLP):** Sentiment analysis, machine translation, text generation.\n",
        "  - **Speech Recognition:** Processing continuous speech signals.\n",
        "  - **Time-Series Forecasting:** Predicting stock prices, weather forecasting, etc.\n",
        "  - **Anomaly Detection:** Identifying unusual patterns in data over time.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 **How an LSTM Works (Step by Step)**  \n",
        "\n",
        "An **LSTM (Long Short-Term Memory)** is a special type of neural network that helps remember important information and forget unnecessary details. It does this using **three gates**:  \n",
        "\n",
        "### 1️⃣ **Forget Gate (Decides what to forget)**  \n",
        "👉 The LSTM first looks at the previous output and the current input. It decides what **old information** is not important anymore and should be forgotten.  \n",
        "\n",
        "💡 **Think of it like cleaning your memory:**  \n",
        "_\"Do I still need this old info, or can I forget it?\"_  \n",
        "\n",
        "---\n",
        "\n",
        "### 2️⃣ **Input Gate (Decides what to learn)**  \n",
        "👉 Next, the LSTM decides what **new information** should be stored.  \n",
        "\n",
        "💡 **Think of it like learning new things:**  \n",
        "_\"Is this new information useful enough to remember?\"_  \n",
        "\n",
        "---\n",
        "\n",
        "### 3️⃣ **Memory Update (Update what’s stored)**  \n",
        "👉 The LSTM updates its memory by combining:  \n",
        "- The important old information (after forgetting unnecessary parts).  \n",
        "- The important new information (after filtering what to keep).  \n",
        "\n",
        "💡 **Think of it like updating a notebook:**  \n",
        "_\"Keep the useful old notes, add the important new ones.\"_  \n",
        "\n",
        "---\n",
        "\n",
        "### 4️⃣ **Output Gate (Decides what to pass forward)**  \n",
        "👉 Finally, the LSTM decides what part of the memory should be sent as **output** for the next step.  \n",
        "\n",
        "💡 **Think of it like answering a question:**  \n",
        "_\"Based on what I know, what should I say next?\"_  \n",
        "\n",
        "---\n",
        "\n",
        "### 🔥 **Final Takeaway:**  \n",
        "LSTM is like a smart assistant:  \n",
        "✅ It **remembers** important things.  \n",
        "❌ It **forgets** what’s not needed.  \n",
        "📝 It **updates** its knowledge.  \n",
        "📢 It **outputs** relevant information.  \n",
        "\n",
        "Does this explanation make sense? 😊\n",
        "\n",
        "### **7. Advantages and Disadvantages**\n",
        "\n",
        "#### **Advantages**\n",
        "- Retains long-term dependencies.\n",
        "- Effective for sequential data (text, time-series).\n",
        "- Handles vanishing gradient problem.\n",
        "\n",
        "#### **Disadvantages**\n",
        "- Computationally intensive.\n",
        "- Longer training times.\n",
        "- May overfit without proper regularization.\n"
      ],
      "metadata": {
        "id": "BpoKu3XrquP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/IMDB Dataset.csv\")"
      ],
      "metadata": {
        "id": "MB-3tzukqrEN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76Iokopxtmkr",
        "outputId": "ea82c3a7-cbe9-4946-d077-269d212d8d21"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Explanation**\n",
        "\n",
        "1. **Padding**: Ensures all reviews are of the same length (100 words).\n",
        "2. **Result**: Each review is now represented as a 2D array of shape `(number of reviews, maxlen)`.\n"
      ],
      "metadata": {
        "id": "VVLX6YuZrG1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "UEp9uXyhtnfX",
        "outputId": "fceab69d-4a97-47be-a1d5-d5d320a4166f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-38b66719-a796-4390-83fb-3066e30a149a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38b66719-a796-4390-83fb-3066e30a149a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-38b66719-a796-4390-83fb-3066e30a149a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-38b66719-a796-4390-83fb-3066e30a149a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1a766cec-c66c-4a94-8f4f-a2e5b6643d63\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1a766cec-c66c-4a94-8f4f-a2e5b6643d63')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1a766cec-c66c-4a94-8f4f-a2e5b6643d63 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49582,\n        \"samples\": [\n          \"\\\"Soul Plane\\\" is a horrible attempt at comedy that only should appeal people with thick skulls, bloodshot eyes and furry pawns. <br /><br />The plot is not only incoherent but also non-existent, acting is mostly sub sub-par with a gang of highly moronic and dreadful characters thrown in for bad measure, jokes are often spotted miles ahead and almost never even a bit amusing. This movie lacks any structure and is full of racial stereotypes that must have seemed old even in the fifties, the only thing it really has going for it is some pretty ladies, but really, if you want that you can rent something from the \\\"Adult\\\" section. OK?<br /><br />I can hardly see anything here to recommend since you'll probably have a lot a better and productive time chasing rats with a sledgehammer or inventing waterproof teabags or whatever.<br /><br />2/10\",\n          \"Guest from the Future tells a fascinating story of time travel, friendship, battle of good and evil -- all with a small budget, child actors, and few special effects. Something for Spielberg and Lucas to learn from. ;) A sixth-grader Kolya \\\"Nick\\\" Gerasimov finds a time machine in the basement of a decrepit building and travels 100 years into the future. He discovers a near-perfect, utopian society where robots play guitars and write poetry, everyone is kind to each other and people enjoy everything technology has to offer. Alice is the daughter of a prominent scientist who invented a device called Mielophone that allows to read minds of humans and animals. The device can be put to both good and bad use, depending on whose hands it falls into. When two evil space pirates from Saturn who want to rule the universe attempt to steal Mielophone, it falls into the hands of 20th century school boy Nick. With the pirates hot on his tracks, he travels back to his time, followed by the pirates, and Alice. Chaos, confusion and funny situations follow as the luckless pirates try to blend in with the earthlings. Alice enrolls in the same school Nick goes to and demonstrates superhuman abilities in PE class. The catch is, Alice doesn't know what Nick looks like, while the pirates do. Also, the pirates are able to change their appearance and turn literally into anyone. (Hmm, I wonder if this is where James Cameron got the idea for Terminator...) Who gets to Nick -- and Mielophone -- first? Excellent plot, non-stop adventures, and great soundtrack. I wish Hollywood made kid movies like this one...\",\n          \"\\\"National Treasure\\\" (2004) is a thoroughly misguided hodge-podge of plot entanglements that borrow from nearly every cloak and dagger government conspiracy clich\\u00e9 that has ever been written. The film stars Nicholas Cage as Benjamin Franklin Gates (how precious is that, I ask you?); a seemingly normal fellow who, for no other reason than being of a lineage of like-minded misguided fortune hunters, decides to steal a 'national treasure' that has been hidden by the United States founding fathers. After a bit of subtext and background that plays laughably (unintentionally) like Indiana Jones meets The Patriot, the film degenerates into one misguided whimsy after another \\u0096 attempting to create a 'Stanley Goodspeed' regurgitation of Nicholas Cage and launch the whole convoluted mess forward with a series of high octane, but disconnected misadventures.<br /><br />The relevancy and logic to having George Washington and his motley crew of patriots burying a king's ransom someplace on native soil, and then, going through the meticulous plan of leaving clues scattered throughout U.S. currency art work, is something that director Jon Turteltaub never quite gets around to explaining. Couldn't Washington found better usage for such wealth during the start up of the country? Hence, we are left with a mystery built on top of an enigma that is already on shaky ground by the time Ben appoints himself the new custodian of this untold wealth. Ben's intentions are noble \\u0096 if confusing. He's set on protecting the treasure. For who and when?\\u0085your guess is as good as mine.<br /><br />But there are a few problems with Ben's crusade. First up, his friend, Ian Holmes (Sean Bean) decides that he can't wait for Ben to make up his mind about stealing the Declaration of Independence from the National Archives (oh, yeah \\u0096 brilliant idea!). Presumably, the back of that famous document holds the secret answer to the ultimate fortune. So Ian tries to kill Ben. The assassination attempt is, of course, unsuccessful, if overly melodramatic. It also affords Ben the opportunity to pick up, and pick on, the very sultry curator of the archives, Abigail Chase (Diane Kruger). She thinks Ben is clearly a nut \\u0096 at least at the beginning. But true to action/romance form, Abby's resolve melts quicker than you can say, \\\"is that the Hope Diamond?\\\" The film moves into full X-File-ish mode, as the FBI, mistakenly believing that Ben is behind the theft, retaliate in various benign ways that lead to a multi-layering of action sequences reminiscent of Mission Impossible meets The Fugitive. Honestly, don't those guys ever get 'intelligence' information that is correct? In the final analysis, \\\"National Treasure\\\" isn't great film making, so much as it's a patchwork rehash of tired old bits from other movies, woven together from scraps, the likes of which would make IL' Betsy Ross blush.<br /><br />The Buena Vista DVD delivers a far more generous treatment than this film is deserving of. The anamorphic widescreen picture exhibits a very smooth and finely detailed image with very rich colors, natural flesh tones, solid blacks and clean whites. The stylized image is also free of blemishes and digital enhancements. The audio is 5.1 and delivers a nice sonic boom to your side and rear speakers with intensity and realism. Extras include a host of promotional junket material that is rather deep and over the top in its explanation of how and why this film was made. If only, as an audience, we had had more clarification as to why Ben and co. were chasing after an illusive treasure, this might have been one good flick. Extras conclude with the theatrical trailer, audio commentary and deleted scenes. Not for the faint-hearted \\u0096 just the thick-headed.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"sentiment\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "-Q8vsfIAtrzT",
        "outputId": "807fc4af-7a49-4de4-d80f-673d3d9b7930"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentiment\n",
              "positive    25000\n",
              "negative    25000\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentiment</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.replace({\"sentiment\": {\"positive\": 1, \"negative\": 0}}, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQKAm4WftuMs",
        "outputId": "57e22a5b-ae6d-4f68-8d78-3c2725bbb719"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-44-c1a779bd745a>:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  data.replace({\"sentiment\": {\"positive\": 1, \"negative\": 0}}, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split data into training data and test data\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "zmC0gQM4t1sC"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.shape)\n",
        "print(test_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO7vItQlt2uU",
        "outputId": "abad40dc-c0cf-4b7d-f941-03c066ba259d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(40000, 2)\n",
            "(10000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize text data\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(train_data[\"review\"])\n",
        "X_train = pad_sequences(tokenizer.texts_to_sequences(train_data[\"review\"]), maxlen=200)\n",
        "X_test = pad_sequences(tokenizer.texts_to_sequences(test_data[\"review\"]), maxlen=200)"
      ],
      "metadata": {
        "id": "cMgWBww0t3zO"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s your explanation in a single Markdown cell:  \n",
        "\n",
        "```markdown\n",
        "## 📌 Tokenizing and Padding Text Data\n",
        "\n",
        "This code processes text reviews by converting them into numerical sequences suitable for deep learning models.\n",
        "\n",
        "### 🔹 Step 1: Initialize Tokenizer  \n",
        "We create a `Tokenizer` to process text data, keeping only the **top 5,000 most frequent words** to reduce memory usage.  \n",
        "```python\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "```\n",
        "\n",
        "### 🔹 Step 2: Fit Tokenizer on Training Data  \n",
        "The tokenizer learns the vocabulary from the training dataset and creates a **word-to-index mapping**.  \n",
        "Example: `\"good movie\"` → `{ \"good\": 1, \"movie\": 2 }`  \n",
        "```python\n",
        "tokenizer.fit_on_texts(train_data[\"review\"])\n",
        "```\n",
        "\n",
        "### 🔹 Step 3: Convert Text to Sequences  \n",
        "Each review is transformed into a sequence of numbers based on the learned vocabulary.  \n",
        "Example: `\"good movie\"` → `[1, 2]`  \n",
        "```python\n",
        "X_train = tokenizer.texts_to_sequences(train_data[\"review\"])\n",
        "X_test = tokenizer.texts_to_sequences(test_data[\"review\"])\n",
        "```\n",
        "\n",
        "### 🔹 Step 4: Pad Sequences  \n",
        "Since reviews have different lengths, we **pad** or **truncate** them to a fixed length of **200 words**.  \n",
        "- **Short reviews** get padded with zeros at the beginning.  \n",
        "- **Long reviews** get truncated from the start.  \n",
        "This ensures all inputs have the same shape for the LSTM model.  \n",
        "```python\n",
        "X_train = pad_sequences(X_train, maxlen=200)\n",
        "X_test = pad_sequences(X_test, maxlen=200)\n",
        "```\n",
        "\n",
        "✅ **Final Output:** `X_train` and `X_test` now contain **numerical representations** of the text reviews, ready for deep learning! 🚀  \n",
        "```\n",
        "\n",
        "This keeps everything concise and well-structured in a single Markdown cell. Let me know if you need tweaks! 😊"
      ],
      "metadata": {
        "id": "KL49HLkBy8k5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmpByr03t5E6",
        "outputId": "5ec29858-5bde-44b3-dbcd-98c5155c6574"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1935    1 1200 ...  205  351 3856]\n",
            " [   3 1651  595 ...   89  103    9]\n",
            " [   0    0    0 ...    2  710   62]\n",
            " ...\n",
            " [   0    0    0 ... 1641    2  603]\n",
            " [   0    0    0 ...  245  103  125]\n",
            " [   0    0    0 ...   70   73 2062]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw-pNqaqt6cE",
        "outputId": "52f5ef3f-4011-4749-bb7a-6bdf6c708c64"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   0    0    0 ...  995  719  155]\n",
            " [  12  162   59 ...  380    7    7]\n",
            " [   0    0    0 ...   50 1088   96]\n",
            " ...\n",
            " [   0    0    0 ...  125  200 3241]\n",
            " [   0    0    0 ... 1066    1 2305]\n",
            " [   0    0    0 ...    1  332   27]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train = train_data[\"sentiment\"]\n",
        "Y_test = test_data[\"sentiment\"]"
      ],
      "metadata": {
        "id": "Z-CBrtYFt7l6"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LyOPLk2t9V1",
        "outputId": "d85afc37-8120-430d-81b6-cfde56a0d6d7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "39087    0\n",
            "30893    0\n",
            "45278    1\n",
            "16398    0\n",
            "13653    0\n",
            "        ..\n",
            "11284    1\n",
            "44732    1\n",
            "38158    0\n",
            "860      1\n",
            "15795    1\n",
            "Name: sentiment, Length: 40000, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM - Long Short-Term Memory**"
      ],
      "metadata": {
        "id": "K98HPoYIuCwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build the model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=5000, output_dim=128, input_length=200))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7GrVOaHuAaT",
        "outputId": "bd31c6a7-5ab6-4315-eb0b-e6ea66463678"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s the explanation for your model in a single Markdown cell:  \n",
        "\n",
        "```markdown\n",
        "## 📌 Building the LSTM Model\n",
        "\n",
        "This code defines a **Sequential** model using an **LSTM (Long Short-Term Memory)** layer for text classification.\n",
        "\n",
        "### 🔹 Step 1: Initialize the Model  \n",
        "We use `Sequential()`, which allows us to stack layers one after another.  \n",
        "```python\n",
        "model = Sequential()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Step 2: Add an Embedding Layer  \n",
        "The **Embedding layer** converts words into dense vector representations.  \n",
        "- `input_dim=5000`: The vocabulary size (top 5,000 most frequent words).  \n",
        "- `output_dim=128`: Each word is represented as a **128-dimensional vector**.  \n",
        "- `input_length=200`: Each review has a fixed length of **200 words**.  \n",
        "\n",
        "```python\n",
        "model.add(Embedding(input_dim=5000, output_dim=128, input_length=200))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Step 3: Add an LSTM Layer  \n",
        "The **LSTM layer** processes the sequence of word embeddings and captures long-term dependencies in text.  \n",
        "- `128`: The number of LSTM units (neurons).  \n",
        "- `dropout=0.2`: Prevents overfitting by randomly setting 20% of inputs to zero during training.  \n",
        "- `recurrent_dropout=0.2`: Adds dropout to the **recurrent** connections within the LSTM cell.  \n",
        "\n",
        "```python\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Step 4: Add a Dense Output Layer  \n",
        "- `Dense(1)`: A **fully connected layer** with 1 neuron, since we’re performing **binary classification**.  \n",
        "- `activation=\"sigmoid\"`: The **sigmoid activation function** ensures the output is a probability (between 0 and 1).  \n",
        "\n",
        "```python\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "```\n",
        "\n",
        "✅ **Final Output:** This LSTM model is now ready to be compiled and trained for text classification! 🚀  \n",
        "```\n",
        "\n",
        "This Markdown cell keeps it concise while explaining each part in detail. Let me know if you need any refinements! 😊"
      ],
      "metadata": {
        "id": "mzIuksLCzLVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "AeBdFXK3uHPO"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "9EhWP8Z6wfir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train, epochs=5, batch_size=64, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENJ2ygSHuRwv",
        "outputId": "1510bc1c-fe2b-45b9-a2da-4c41cdb6ab8d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 319ms/step - accuracy: 0.7176 - loss: 0.5359 - val_accuracy: 0.8511 - val_loss: 0.3513\n",
            "Epoch 2/5\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 320ms/step - accuracy: 0.8590 - loss: 0.3492 - val_accuracy: 0.8466 - val_loss: 0.3598\n",
            "Epoch 3/5\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 318ms/step - accuracy: 0.8783 - loss: 0.3038 - val_accuracy: 0.8652 - val_loss: 0.3217\n",
            "Epoch 4/5\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 313ms/step - accuracy: 0.8913 - loss: 0.2736 - val_accuracy: 0.8704 - val_loss: 0.3278\n",
            "Epoch 5/5\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 307ms/step - accuracy: 0.9053 - loss: 0.2429 - val_accuracy: 0.8731 - val_loss: 0.3160\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7827c75ee4d0>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, Y_test)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk4e3wL_vM06",
        "outputId": "bf70d251-aa16-44b1-b6a8-21a06aa7fcab"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 62ms/step - accuracy: 0.8714 - loss: 0.3066\n",
            "Test Loss: 0.30455735325813293\n",
            "Test Accuracy: 0.8755999803543091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(review):\n",
        "  # tokenize and pad the review\n",
        "  sequence = tokenizer.texts_to_sequences([review])\n",
        "  padded_sequence = pad_sequences(sequence, maxlen=200)\n",
        "  prediction = model.predict(padded_sequence)\n",
        "  sentiment = \"positive\" if prediction[0][0] > 0.5 else \"negative\"\n",
        "  return sentiment"
      ],
      "metadata": {
        "id": "75uwkPWfvjAB"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```markdown\n",
        "## 📌 Sentiment Analysis Using LSTM  \n",
        "\n",
        "This code defines a function to predict whether a given review has a **positive** or **negative** sentiment using a trained LSTM model.  \n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Step 1: Tokenize the Input Review  \n",
        "- The review is converted into a sequence of numbers using the same tokenizer that was trained earlier.  \n",
        "- `texts_to_sequences([review])` maps words to their corresponding indices.  \n",
        "\n",
        "```python\n",
        "sequence = tokenizer.texts_to_sequences([review])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Step 2: Pad the Sequence  \n",
        "- The sequence is padded to ensure it has a fixed length of **200 words** (same as the training data).  \n",
        "- If the review is **shorter**, it gets padded with zeros.  \n",
        "- If the review is **longer**, it gets truncated.  \n",
        "\n",
        "```python\n",
        "padded_sequence = pad_sequences(sequence, maxlen=200)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Step 3: Predict Sentiment  \n",
        "- The **trained LSTM model** predicts a probability between 0 and 1.  \n",
        "- `model.predict(padded_sequence)` returns a probability score.  \n",
        "\n",
        "```python\n",
        "prediction = model.predict(padded_sequence)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Step 4: Interpret the Prediction  \n",
        "- If the probability is **greater than 0.5**, the review is classified as **\"positive\"**.  \n",
        "- Otherwise, it is classified as **\"negative\"**.  \n",
        "\n",
        "```python\n",
        "sentiment = \"positive\" if prediction[0][0] > 0.5 else \"negative\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Complete Function  \n",
        "```python\n",
        "def predict_sentiment(review):\n",
        "    # Tokenize and pad the review\n",
        "    sequence = tokenizer.texts_to_sequences([review])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=200)\n",
        "    \n",
        "    # Predict sentiment\n",
        "    prediction = model.predict(padded_sequence)\n",
        "    sentiment = \"positive\" if prediction[0][0] > 0.5 else \"negative\"\n",
        "    \n",
        "    return sentiment\n",
        "```\n",
        "\n",
        "✅ **Final Output:** The function returns **\"positive\"** or **\"negative\"** based on the predicted sentiment! 🚀  \n",
        "```\n",
        "\n",
        "This Markdown cell includes **everything** in a structured and well-explained format. Let me know if you need modifications! 😊"
      ],
      "metadata": {
        "id": "WQj9VB8szlYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example usage\n",
        "new_review = \"This movie was fantastic. I loved it.\"\n",
        "sentiment = predict_sentiment(new_review)\n",
        "print(f\"The sentiment of the review is: {sentiment}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hi_bePyvjCU",
        "outputId": "d52e0af5-ab56-44c7-b89d-80401beccd68"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step\n",
            "The sentiment of the review is: positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example usage\n",
        "new_review = \"This movie was not that good\"\n",
        "sentiment = predict_sentiment(new_review)\n",
        "print(f\"The sentiment of the review is: {sentiment}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5da3z-YvjE7",
        "outputId": "ff2c4efa-9c5d-4a50-ad48-6503fb6cb6e9"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "The sentiment of the review is: negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example usage\n",
        "new_review = \"This movie was ok but not that good.\"\n",
        "sentiment = predict_sentiment(new_review)\n",
        "print(f\"The sentiment of the review is: {sentiment}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htf8omf4vjIM",
        "outputId": "50ab5814-685c-4a25-a53e-fd45bc920ec0"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "The sentiment of the review is: negative\n"
          ]
        }
      ]
    }
  ]
}